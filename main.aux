\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Abstract}{2}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Introduction}{5}{chapter.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{9}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Related Work}{9}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Near Infrared Spectroscopy (NIRS)}{9}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Liquid Chromatography Mass Spectrometry}{9}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}R Programming}{9}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Machine Learning}{9}{section.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Example near-infrared spectra illustrating reflectance across wavelengths. Each line represents a unique sample, showing how NIRS captures molecular absorption/reflectance patterns.}}{10}{figure.2.1}\protected@file@percent }
\newlabel{Figure 5}{{2.1}{10}{Example near-infrared spectra illustrating reflectance across wavelengths. Each line represents a unique sample, showing how NIRS captures molecular absorption/reflectance patterns}{figure.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Different machine learning models illustrating the four primary learning methods: supervised, unsupervised, semi-supervised, and reinforcement learning.}}{11}{figure.2.2}\protected@file@percent }
\newlabel{Figure 1}{{2.2}{11}{Different machine learning models illustrating the four primary learning methods: supervised, unsupervised, semi-supervised, and reinforcement learning}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Partial Least Square Regression (PLSR)}{12}{subsection.2.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A comparison of PCA (A) and PLS (B). In the PCA plot, the x-axis represents a combination of variables (e.g., three metabolites) that captures the greatest variation in the dataset, independent of group classification. In contrast, PLS focuses on explaining the relationship with an explanatory variable, such as "Treatment" in this example}}{12}{figure.2.3}\protected@file@percent }
\newlabel{Figure 2}{{2.3}{12}{A comparison of PCA (A) and PLS (B). In the PCA plot, the x-axis represents a combination of variables (e.g., three metabolites) that captures the greatest variation in the dataset, independent of group classification. In contrast, PLS focuses on explaining the relationship with an explanatory variable, such as "Treatment" in this example}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Random Forest (RF)}{13}{subsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Schematic representation of a Random Forest model.}}{13}{figure.2.4}\protected@file@percent }
\newlabel{Figure 3}{{2.4}{13}{Schematic representation of a Random Forest model}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Convolutional Neural Network (CNN)}{14}{subsection.2.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Comparison of artificial neurons and real neurons}}{14}{figure.2.5}\protected@file@percent }
\newlabel{Figure 4}{{2.5}{14}{Comparison of artificial neurons and real neurons}{figure.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}High Perfomance Computing}{15}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Implementation}{16}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Packages}{16}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Contributions elsewhere}{16}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}HPC runs}{16}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results and Discussion}{17}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Data charecterestics}{17}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Baseline Machine Learning Models Pablo}{17}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Variable importance}{17}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3} Variations in Baseline systems}{17}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1} modifying the Test and Training split}{17}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2} input data length}{17}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Sues}{17}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Reference}{18}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\gdef \@abspage@last{21}
